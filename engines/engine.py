"""
engine.py - Ù…Ø­Ø±Ùƒ v17.7 (Smart Validator)
- Ù…ÙŠØ²Ø©: Ø·Ø¨Ù‚Ø© ØªØ­Ù‚Ù‚ Ø°ÙƒÙŠØ© Ù„Ø±ÙØ¶ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ© (Ø¹ÙŠÙ†Ø§ØªØŒ Ù…Ø§Ø±ÙƒØ§Øª Ù…Ø®ØªÙ„ÙØ©).
- ØªØ­Ø³ÙŠÙ†: ÙØ­Øµ ÙØ±ÙˆÙ‚Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¶Ø®Ù…Ø©.
"""
import re
import pandas as pd
import io
from rapidfuzz import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from config import (MATCH_THRESHOLD, HIGH_CONFIDENCE, PRICE_TOLERANCE, 
                    REJECT_KEYWORDS, KNOWN_BRANDS, WORD_REPLACEMENTS)

# ===== Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© =====
def normalize(text):
    if not isinstance(text, str): return ""
    t = text.strip().lower()
    # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø£Ø­Ø±Ù ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
    t = re.sub(r'[^\w\s\u0600-\u06FF.]', ' ', t)
    for ar, en in WORD_REPLACEMENTS.items():
        if ar in t: t = t.replace(ar.lower(), en)
    return re.sub(r'\s+', ' ', t).strip()

def extract_size(text):
    if not isinstance(text, str): return 0
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø¬Ù… Ø¨Ø¯Ù‚Ø© (ml, g, oz)
    m = re.search(r'(\d+(?:\.\d+)?)\s*(?:ml|lz|oz|Ù…Ù„|Ù…Ù„ÙŠ|g|gm|gram)', text.lower())
    return float(m.group(1)) if m else 0

def extract_brand(text):
    if not isinstance(text, str): return ""
    tl = text.lower()
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø§Ø±ÙƒØ§Øª Ø¨Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
    sorted_brands = sorted(KNOWN_BRANDS, key=len, reverse=True)
    for b in sorted_brands:
        if b.lower() in tl: return b.lower()
    return ""

def is_sample_or_tester(text):
    """ÙƒØ´Ù Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø¹ÙŠÙ†Ø§Øª ÙˆØ§Ù„ØªØ³ØªØ±Ø§Øª"""
    if not isinstance(text, str): return False
    t = text.lower()
    sample_words = ['sample', 'vial', 'Ø¹ÙŠÙ†Ø©', 'Ø¹ÙŠÙŠÙ†Ø©', 'ØªØ¬Ø±Ø¨Ø©', '2ml', '1ml', '1.5ml']
    return any(w in t for w in sample_words)

# ===== Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ =====
class SmartMatcher:
    def __init__(self, our_df, comp_dfs, mapping=None):
        self.our_df = our_df.copy()
        self.comp_dfs = comp_dfs
        self.mapping = mapping
        # Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙŠ
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb', 
            ngram_range=(3, 5), # Ø²ÙŠØ§Ø¯ØªÙ‡Ø§ Ù„ØªÙ‚Ù„ÙŠÙ„ ØªØ£Ø«ÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
            min_df=1
        )
        self.prepare_data()

    def prepare_data(self):
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        if self.mapping:
            self.our_prod_col = self.mapping.get('our_name')
            self.our_price_col = self.mapping.get('our_price')
        else:
            self.our_prod_col = self.our_df.columns[0]
            self.our_price_col = self.our_df.columns[1]

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§
        self.our_df[self.our_prod_col] = self.our_df[self.our_prod_col].astype(str)
        self.our_df['norm_name'] = self.our_df[self.our_prod_col].apply(normalize)
        self.our_df['brand'] = self.our_df[self.our_prod_col].apply(extract_brand)
        self.our_df['size'] = self.our_df[self.our_prod_col].apply(extract_size)
        self.our_df['is_sample'] = self.our_df[self.our_prod_col].apply(is_sample_or_tester)

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
        self.processed_comps = {}
        for name, df in self.comp_dfs.items():
            df_clean = df.copy()
            if self.mapping:
                target_p = self.mapping.get('comp_name')
                target_pr = self.mapping.get('comp_price')
                p_col = target_p if target_p in df_clean.columns else df_clean.columns[0]
                pr_col = target_pr if target_pr in df_clean.columns else df_clean.columns[1]
            else:
                p_col = df_clean.columns[0]
                pr_col = df_clean.columns[1]

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¹Ø±
            if pr_col in df_clean.columns:
                df_clean[pr_col] = pd.to_numeric(
                    df_clean[pr_col].astype(str).str.replace(r'[^\d.]', '', regex=True), 
                    errors='coerce'
                ).fillna(0)
            
            df_clean[p_col] = df_clean[p_col].astype(str)
            df_clean['norm_name'] = df_clean[p_col].apply(normalize)
            df_clean['brand'] = df_clean[p_col].apply(extract_brand)
            df_clean['size'] = df_clean[p_col].apply(extract_size)
            df_clean['is_sample'] = df_clean[p_col].apply(is_sample_or_tester)
            
            self.processed_comps[name] = {'df': df_clean, 'p_col': p_col, 'pr_col': pr_col}

    def validate_match(self, row_our, row_comp, price_our, price_comp):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø°ÙƒÙŠ Ù‚Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        # 1. Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ù…Ø§Ø±ÙƒØ© (Ù‚Ø§ØªÙ„)
        if row_our['brand'] and row_comp['brand']:
            if row_our['brand'] != row_comp['brand']:
                # ØªØ³Ø§Ù‡Ù„ Ø¨Ø³ÙŠØ· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¥Ø­Ø¯Ø§Ù‡Ù…Ø§ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø£Ø®Ø±Ù‰
                if row_our['brand'] not in row_comp['brand'] and row_comp['brand'] not in row_our['brand']:
                    return False, "Brand Mismatch"

        # 2. ÙØ® Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (Sample Trap)
        # Ø¥Ø°Ø§ Ù…Ù†ØªØ¬Ù†Ø§ Ù„ÙŠØ³ Ø¹ÙŠÙ†Ø© ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ Ø¹ÙŠÙ†Ø© (Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³)
        if row_our['is_sample'] != row_comp['is_sample']:
            return False, "Sample vs Full Mismatch"

        # 3. ÙƒØ§Ø±Ø«Ø© Ø§Ù„Ø³Ø¹Ø± (Price Logic)
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙØ±Ù‚ Ù‡Ø§Ø¦Ù„ (Ø£ÙƒØ«Ø± Ù…Ù† 3 Ø£Ø¶Ø¹Ø§Ù Ø£Ùˆ Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø«Ù„Ø«)
        if price_our > 0 and price_comp > 0:
            ratio = price_our / price_comp
            if ratio > 4.0: # Ù…Ù†ØªØ¬Ù†Ø§ Ø¨Ù€ 400 ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ Ø¨Ù€ 100 (ØºØ§Ù„Ø¨Ø§Ù‹ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ø¬Ù… Ø£Ùˆ Ø§Ù„Ù†ÙˆØ¹)
                return False, "Price too low (Likely Sample/Fake)"
            if ratio < 0.25: # Ù…Ù†ØªØ¬Ù†Ø§ Ø¨Ù€ 100 ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ Ø¨Ù€ 400 (ØºØ§Ù„Ø¨Ø§Ù‹ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©)
                return False, "Price too high (Likely Set/Bundle)"

        # 4. Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø­Ø¬Ù… (Size Mismatch)
        if row_our['size'] > 0 and row_comp['size'] > 0:
            if row_our['size'] != row_comp['size']:
                return False, "Size Mismatch"

        return True, "Valid"

    def strict_score(self, row_our, row_comp):
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†ØµÙŠ
        score = fuzz.token_sort_ratio(row_our['norm_name'], row_comp['norm_name'])
        
        # ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù…Ø©
        if row_our['brand'] == row_comp['brand'] and row_our['brand'] != "":
            score += 10 # Ù…ÙƒØ§ÙØ£Ø© ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø§Ø±ÙƒØ©
        
        return score

    def run(self, progress_callback=None):
        results = []
        expected_cols = ["Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ù„Ø³Ø¹Ø±", "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³", "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³", "Ø§Ù„ÙØ±Ù‚", 
                         "Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚", "Ø§Ù„Ù‚Ø±Ø§Ø±", "Ø§Ù„Ù…Ù†Ø§ÙØ³", "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"]

        if not self.processed_comps: return pd.DataFrame(columns=expected_cols)

        total = len(self.our_df)
        
        for comp_name, comp_data in self.processed_comps.items():
            comp_df = comp_data['df']
            if comp_df.empty: continue
            
            try:
                tfidf_comp = self.vectorizer.fit_transform(comp_df['norm_name'])
                tfidf_our = self.vectorizer.transform(self.our_df['norm_name'])
                cosine_sim = cosine_similarity(tfidf_our, tfidf_comp)
            except: continue

            for idx, row_our in self.our_df.iterrows():
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø·Ø±ÙÙ†Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
                # if row_our['is_sample']: continue
                
                scores = cosine_sim[idx]
                top_idx = scores.argsort()[-5:][::-1]
                
                best_match = None
                best_score = 0
                
                for c_idx in top_idx:
                    if scores[c_idx] < 0.2: continue # Ø¹ØªØ¨Ø© Ø£ÙˆÙ„ÙŠØ©
                    
                    row_comp = comp_df.iloc[c_idx]
                    p_our = float(row_our[self.our_price_col]) if self.our_price_col in row_our else 0
                    p_comp = float(row_comp[comp_data['pr_col']])

                    # --- Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø°ÙƒÙŠ (Smart Validation) ---
                    is_valid, reason = self.validate_match(row_our, row_comp, p_our, p_comp)
                    if not is_valid:
                        continue # ØªØ®Ø·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø±Ø´Ø­ Ø§Ù„ÙØ§Ø³Ø¯

                    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
                    s = self.strict_score(row_our, row_comp)
                    
                    if s > best_score:
                        best_score = s
                        best_match = row_comp

                if best_match is not None and best_score >= MATCH_THRESHOLD:
                    p_comp = float(best_match[comp_data['pr_col']])
                    p_our = float(row_our[self.our_price_col]) if self.our_price_col in row_our else 0
                    diff = p_our - p_comp
                    
                    decision = "âœ… Ù…ÙˆØ§ÙÙ‚"
                    if diff > PRICE_TOLERANCE: decision = "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰"
                    elif diff < -PRICE_TOLERANCE: decision = "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„"
                    elif best_score < HIGH_CONFIDENCE: decision = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"
                    
                    results.append({
                        "Ø§Ù„Ù…Ù†ØªØ¬": row_our[self.our_prod_col],
                        "Ø§Ù„Ø³Ø¹Ø±": p_our,
                        "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": best_match[comp_data['p_col']],
                        "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³": p_comp,
                        "Ø§Ù„ÙØ±Ù‚": diff,
                        "Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚": best_score,
                        "Ø§Ù„Ù‚Ø±Ø§Ø±": decision,
                        "Ø§Ù„Ù…Ù†Ø§ÙØ³": comp_name
                    })
                
                if progress_callback and idx % 50 == 0: progress_callback(idx/total)

        if not results: return pd.DataFrame(columns=expected_cols)
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ Ù„ÙƒÙ„ Ù…Ù†ØªØ¬
        df = pd.DataFrame(results)
        final = []
        for n, g in df.groupby("Ø§Ù„Ù…Ù†ØªØ¬"):
            # Ù†ÙØ¶Ù„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ Ø«Ù… Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ù…Ù†Ø§ÙØ³
            best = g.sort_values(by=['Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚', 'Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³'], ascending=[False, True]).iloc[0].to_dict()
            best['Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†'] = g.to_dict('records')
            final.append(best)
            
        return pd.DataFrame(final)

# ===== ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ØªØµØ¯ÙŠØ± =====
def read_file(uploaded_file):
    uploaded_file.seek(0)
    name = uploaded_file.name.lower()
    df = None
    encodings = ['utf-8', 'utf-8-sig', 'windows-1256', 'cp1256']
    
    if name.endswith('.csv'):
        for enc in encodings:
            try:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=enc)
                if not df.empty: break
            except: continue
    elif name.endswith(('.xlsx', '.xls')):
        try: df = pd.read_excel(uploaded_file)
        except: pass
        
    if df is not None:
        df.columns = df.columns.str.strip()
        df = df.dropna(how='all')
    return df, None

def run_full_analysis(our_df, comp_dfs, progress_callback=None, mapping=None):
    matcher = SmartMatcher(our_df, comp_dfs, mapping)
    return matcher.run(progress_callback)

def find_missing_products(our_df, comp_dfs):
    return pd.DataFrame()

def export_excel(df, sheet_name="Sheet1"):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as w:
        df.to_excel(w, sheet_name=sheet_name[:30], index=False)
    return output.getvalue()

def export_section_excel(df, name): return export_excel(df, name)
