"""
engine.py - Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ† v17.2 (Hybrid Vectorization)
- Ø§Ù„ØªØ±Ù‚ÙŠØ©: TF-IDF Vectorization Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø£ÙˆÙ„ÙŠ (ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©).
- Ø§Ù„Ø¯Ù‚Ø©: Reranking Ù…Ù†Ø·Ù‚ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ø±ÙƒØ© ÙˆØ§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„Ù†ÙˆØ¹.
- Ø§Ù„Ø£Ø¯Ø§Ø¡: Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Vectorized Preprocessing).
"""
import re
import pandas as pd
import numpy as np
import io
from rapidfuzz import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from config import (REJECT_KEYWORDS, KNOWN_BRANDS, WORD_REPLACEMENTS,
                    MATCH_THRESHOLD, HIGH_CONFIDENCE, REVIEW_THRESHOLD,
                    PRICE_TOLERANCE, TESTER_KEYWORDS, SET_KEYWORDS)

# ===== 1. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØªØ­Ù„ÙŠÙ„ (Ù…Ø­Ø³Ù†Ø©) =====

def normalize(text):
    if not isinstance(text, str): return ""
    t = text.strip().lower()
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ù…ÙˆØ³
    for ar, en in WORD_REPLACEMENTS.items():
        if ar in t: # ØªØ­Ù‚Ù‚ Ø³Ø±ÙŠØ¹ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
            t = t.replace(ar.lower(), en)
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ²
    t = re.sub(r'[^\w\s.]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def extract_size(text):
    if not isinstance(text, str): return 0
    # ØªØ­Ø³ÙŠÙ† Regex Ù„ÙŠÙƒÙˆÙ† Ø£Ø¯Ù‚ ÙˆØ£Ø³Ø±Ø¹
    m = re.search(r'(\d+(?:\.\d+)?)\s*(?:ml|lz|Ù…Ù„|Ù…Ù„ÙŠ|g|gram)', text.lower())
    return float(m.group(1)) if m else 0

def extract_brand(text):
    if not isinstance(text, str): return ""
    tl = text.lower()
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø§Ø±ÙƒØ© (Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡)
    for b in KNOWN_BRANDS:
        if b.lower() in tl:
            return b
    return ""

def extract_type(text):
    if not isinstance(text, str): return ""
    tl = text.lower()
    if 'edp' in tl or 'parfum' in tl or 'Ø¨Ø§Ø±ÙØ§Ù†' in tl: return 'edp'
    if 'edt' in tl or 'toilette' in tl or 'ØªÙˆØ§Ù„ÙŠØª' in tl: return 'edt'
    if 'edc' in tl or 'cologne' in tl or 'ÙƒÙˆÙ„ÙˆÙ†' in tl: return 'edc'
    if 'oil' in tl or 'Ø²ÙŠØª' in tl: return 'oil'
    if 'tester' in tl or 'ØªØ³ØªØ±' in tl: return 'tester'
    return ''

def is_sample(text):
    if not isinstance(text, str): return False
    tl = text.lower()
    return any(k in tl for k in REJECT_KEYWORDS)

# ===== 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª =====
def read_file(uploaded_file):
    try:
        name = uploaded_file.name.lower()
        if name.endswith('.csv'):
            try:
                df = pd.read_csv(uploaded_file, encoding='utf-8')
            except:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
        elif name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(uploaded_file)
        else:
            return None, "ØµÙŠØºØ© Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©."
        
        df.columns = df.columns.str.strip()
        df = df.dropna(how='all')
        
        # ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„ÙŠ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        df = df.rename(columns=lambda x: x.lower().replace(' ', '_'))
        return df, None
    except Exception as e:
        return None, f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}"

# ===== 3. ÙØ¦Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø°ÙƒÙŠØ© (Smart Matcher Class) =====

class SmartMatcher:
    def __init__(self, our_df, comp_dfs):
        self.our_df = our_df.copy()
        self.comp_dfs = comp_dfs
        # Ø¥Ø¹Ø¯Ø§Ø¯ Vectorizer Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb', # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù (ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©)
            ngram_range=(2, 4), # ÙŠØ£Ø®Ø° Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø­Ø±ÙÙŠÙ† Ø¥Ù„Ù‰ 4 Ø£Ø­Ø±Ù
            min_df=1
        )
        self.prepare_data()

    def get_col_name(self, df, candidates):
        for c in candidates:
            for col in df.columns:
                if c.lower() in col.lower():
                    return col
        return df.columns[0]

    def prepare_data(self):
        # 1. ØªØ­Ø¯ÙŠØ¯ Ø£Ø¹Ù…Ø¯Ø© Ù…Ù†ØªØ¬Ø§ØªÙ†Ø§
        self.our_prod_col = self.get_col_name(self.our_df, ["product", "name", "Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù…"])
        self.our_price_col = self.get_col_name(self.our_df, ["price", "Ø³Ø¹Ø±", "Ø§Ù„Ø³Ø¹Ø±"])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ (Vectorized)
        self.our_df['norm_name'] = self.our_df[self.our_prod_col].apply(normalize)
        self.our_df['brand'] = self.our_df[self.our_prod_col].apply(extract_brand)
        self.our_df['size'] = self.our_df[self.our_prod_col].apply(extract_size)
        self.our_df['type'] = self.our_df[self.our_prod_col].apply(extract_type)

        # 2. ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†
        self.processed_comps = {}
        for name, df in self.comp_dfs.items():
            df_clean = df.copy()
            p_col = self.get_col_name(df_clean, ["product", "name", "Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ø³Ù…"])
            pr_col = self.get_col_name(df_clean, ["price", "Ø³Ø¹Ø±", "Ø§Ù„Ø³Ø¹Ø±"])
            
            # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ø¨ÙƒØ±Ø§Ù‹
            df_clean = df_clean[~df_clean[p_col].apply(is_sample)]
            
            df_clean['norm_name'] = df_clean[p_col].apply(normalize)
            df_clean['brand'] = df_clean[p_col].apply(extract_brand)
            df_clean['size'] = df_clean[p_col].apply(extract_size)
            df_clean['type'] = df_clean[p_col].apply(extract_type)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¹Ø± Ù„Ø£Ø±Ù‚Ø§Ù…
            df_clean[pr_col] = pd.to_numeric(df_clean[pr_col], errors='coerce').fillna(0)
            
            self.processed_comps[name] = {
                'df': df_clean,
                'p_col': p_col,
                'pr_col': pr_col
            }

    def strict_score(self, row_our, row_comp):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±Ø´ÙŠØ­ Ø§Ù„Ø£ÙˆÙ„ÙŠ"""
        # 1. Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³Ù… (RapidFuzz)
        base_score = fuzz.token_sort_ratio(row_our['norm_name'], row_comp['norm_name'])
        
        # 2. Ù…ÙƒØ§ÙØ£Ø©/Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ù…Ø§Ø±ÙƒØ©
        if row_our['brand'] and row_comp['brand']:
            if row_our['brand'].lower() == row_comp['brand'].lower():
                base_score += 5
            else:
                return 0 # Ù…Ø§Ø±ÙƒØ© Ù…Ø®ØªÙ„ÙØ© = Ø±ÙØ¶ ÙÙˆØ±ÙŠ

        # 3. Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø­Ø¬Ù… (ØµØ§Ø±Ù…Ø©)
        if row_our['size'] > 0 and row_comp['size'] > 0:
            if row_our['size'] != row_comp['size']:
                return 0 # Ø­Ø¬Ù… Ù…Ø®ØªÙ„Ù = Ø±ÙØ¶ ÙÙˆØ±ÙŠ
            else:
                base_score += 5
        
        # 4. Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ù†ÙˆØ¹ (EDP vs EDT)
        if row_our['type'] and row_comp['type']:
            if row_our['type'] != row_comp['type']:
                base_score -= 15

        return min(100, max(0, base_score))

    def run(self, progress_callback=None):
        results = []
        total_items = len(self.our_df)
        
        # Ø­Ù„Ù‚Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ù†Ø§ÙØ³
        for comp_name, comp_data in self.processed_comps.items():
            comp_df = comp_data['df']
            if comp_df.empty: continue
            
            # Ø¨Ù†Ø§Ø¡ TF-IDF Matrix Ù„Ù„Ù…Ù†Ø§ÙØ³
            comp_names = comp_df['norm_name'].tolist()
            try:
                tfidf_matrix_comp = self.vectorizer.fit_transform(comp_names)
            except ValueError: continue

            # ØªØ¬Ù‡ÙŠØ² Ù…Ù†ØªØ¬Ø§ØªÙ†Ø§
            our_names = self.our_df['norm_name'].tolist()
            tfidf_matrix_our = self.vectorizer.transform(our_names)

            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Cosine Similarity) - Ø¹Ù…Ù„ÙŠØ© Ù…ØµÙÙˆÙØ§Øª Ø³Ø±ÙŠØ¹Ø©
            cosine_sim = cosine_similarity(tfidf_matrix_our, tfidf_matrix_comp)

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            for idx, row_our in self.our_df.iterrows():
                if is_sample(row_our[self.our_prod_col]): continue
                
                # Ø£ÙØ¶Ù„ 5 Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ TF-IDF
                sim_scores = cosine_sim[idx]
                top_indices = sim_scores.argsort()[-5:][::-1] 
                
                best_match = None
                best_score = 0

                for comp_idx in top_indices:
                    if sim_scores[comp_idx] < 0.3: continue # ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹
                    
                    row_comp = comp_df.iloc[comp_idx]
                    
                    # Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØµØ§Ø±Ù…
                    score = self.strict_score(row_our, row_comp)
                    
                    if score > best_score and score >= MATCH_THRESHOLD:
                        best_score = score
                        best_match = row_comp

                if best_match is not None:
                    our_price = float(pd.to_numeric(row_our[self.our_price_col], errors='coerce') or 0)
                    comp_price = float(best_match[comp_data['pr_col']])
                    diff = our_price - comp_price if comp_price > 0 else 0
                    
                    decision = "âœ… Ù…ÙˆØ§ÙÙ‚"
                    risk = "Ù…Ù†Ø®ÙØ¶"
                    
                    if diff > PRICE_TOLERANCE:
                        decision = "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰"
                        risk = "Ø¹Ø§Ù„ÙŠ" if diff > 20 else "Ù…ØªÙˆØ³Ø·"
                    elif diff < -PRICE_TOLERANCE:
                        decision = "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„"
                    elif best_score < REVIEW_THRESHOLD:
                        decision = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"
                        risk = "Ù…ØªÙˆØ³Ø·"

                    results.append({
                        "Ø§Ù„Ù…Ù†ØªØ¬": row_our[self.our_prod_col],
                        "Ø§Ù„Ø³Ø¹Ø±": our_price,
                        "Ø§Ù„Ù…Ø§Ø±ÙƒØ©": row_our['brand'],
                        "Ø§Ù„Ø­Ø¬Ù…": row_our['size'],
                        "Ø§Ù„Ù†ÙˆØ¹": row_our['type'],
                        "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": best_match[comp_data['p_col']],
                        "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³": comp_price,
                        "Ø§Ù„ÙØ±Ù‚": round(diff, 2),
                        "Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚": round(best_score, 1),
                        "Ø§Ù„Ù‚Ø±Ø§Ø±": decision,
                        "Ø§Ù„Ø®Ø·ÙˆØ±Ø©": risk,
                        "Ø§Ù„Ù…Ù†Ø§ÙØ³": comp_name
                    })

                if progress_callback and idx % 50 == 0: # ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 50 Ø¹Ù†ØµØ± Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ØºØ·
                    progress_callback((idx + 1) / total_items)

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        df_res = pd.DataFrame(results)
        if df_res.empty: return pd.DataFrame()

        final_rows = []
        grouped = df_res.groupby('Ø§Ù„Ù…Ù†ØªØ¬')
        
        for name, group in grouped:
            valid_comps = group[group['Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³'] > 0]
            if not valid_comps.empty:
                best_comp_row = valid_comps.loc[valid_comps['Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³'].idxmin()].to_dict()
                best_comp_row['Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†'] = group[['Ø§Ù„Ù…Ù†Ø§ÙØ³', 'Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³', 'Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³', 'Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚']].rename(
                    columns={'Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³': 'name', 'Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³': 'price', 'Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚': 'score', 'Ø§Ù„Ù…Ù†Ø§ÙØ³': 'competitor'}
                ).to_dict('records')
                final_rows.append(best_comp_row)
            else:
                row = group.iloc[0].to_dict()
                row['Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†'] = []
                final_rows.append(row)

        return pd.DataFrame(final_rows)

# ===== 4. Ø¯Ø§Ù„Ø© Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© =====
def run_full_analysis(our_df, comp_dfs, progress_callback=None):
    matcher = SmartMatcher(our_df, comp_dfs)
    return matcher.run(progress_callback)

# ===== 5. Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø§Øª =====
def find_missing_products(our_df, comp_dfs):
    missing = []
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ù„Ù…ÙÙ‚ÙˆØ¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Vectorization
    our_clean = our_df.iloc[:, 0].astype(str).apply(normalize).tolist()
    if not our_clean: return pd.DataFrame()

    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=1)
    try:
        tfidf_our = vectorizer.fit_transform(our_clean)
    except: return pd.DataFrame()

    for comp_name, comp_df in comp_dfs.items():
        c_col = comp_df.columns[0]
        for col in comp_df.columns:
            if "Ù…Ù†ØªØ¬" in col or "name" in col.lower(): c_col = col; break
        p_col = comp_df.columns[1]
        for col in comp_df.columns:
            if "Ø³Ø¹Ø±" in col or "price" in col.lower(): p_col = col; break

        comp_names = comp_df[c_col].astype(str).apply(normalize).tolist()
        if not comp_names: continue

        tfidf_comp = vectorizer.transform(comp_names)
        cosine_sim = cosine_similarity(tfidf_comp, tfidf_our)
        max_scores = cosine_sim.max(axis=1)

        for idx, score in enumerate(max_scores):
            if score < 0.65:
                row = comp_df.iloc[idx]
                if is_sample(str(row[c_col])): continue
                missing.append({
                    "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": row[c_col],
                    "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³": pd.to_numeric(row[p_col], errors='coerce'),
                    "Ø§Ù„Ù…Ù†Ø§ÙØ³": comp_name,
                    "Ø§Ù„Ù…Ø§Ø±ÙƒØ©": extract_brand(str(row[c_col])),
                    "Ø§Ù„Ø­Ø¬Ù…": extract_size(str(row[c_col])),
                    "Ø§Ù„Ù†ÙˆØ¹": extract_type(str(row[c_col]))
                })
    
    return pd.DataFrame(missing).drop_duplicates(subset=['Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³'])

# ===== 6. Ø§Ù„ØªØµØ¯ÙŠØ± =====
def export_excel(df, sheet_name="Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
    output = io.BytesIO()
    export_df = df.copy()
    if "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†" in export_df.columns:
        export_df = export_df.drop(columns=["Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"])
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        export_df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
    return output.getvalue()

def export_section_excel(df, section_name):
    return export_excel(df, sheet_name=section_name[:31])
